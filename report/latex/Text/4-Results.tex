\section{Results}

\subsection{Training comparison}

The training phase of a Reinforcement Learning agent and a Genetic Algorithm are fundamentally different.
Therefore, a way to harmonize the training data of the two methods in order to compare them has to be found.
\\
The first modification is to consider RL episodes the same as GA individuals and aggregate the RL performances to match the number of generations used for GA.
For example considering a population size of $k$ for the Genetic Algorithm, the max and mean of every $k$ Reinforcement Learning episodes have been considered to compare them with each GA generation. Here, $k$ = 100.
\\
Second, the runtime per iteration might differ between the two methods. To account for this difference, the performance data was plotted not against the number of iterations but against the training time (in seconds).
Of course both algorithms were run on the same machine for a fair comparison.
\\
Figure \ref{figAVG} and Figure \ref{figMAX} show average and maximum results achieved at each iteration over training time.

\begin{figure}[H]
	\centering
	\includegraphics [scale = 0.5]{Images/RL_GA_comparison_avg.png}
	\caption{Evolution of mean score in time of both the two algorithms.}
	\label{figAVG}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics [scale = 0.5]{Images/RL_GA_comparison_max.png}
	\caption{Evolution of max score in time of both the two algorithms.}
	\label{figMAX}
\end{figure}

As can be seen in Figure \ref{figAVG} and Figure \ref{figMAX}, the first takeaway is that RL significantly outperforms GA.
One interesting point though, is that GA performances seem to grow quicker than their RL counterpart at the beginning of training.
This can be explained by the fact that GA performs parallel search and that the genotype can be modified much more in one crossover than RL's Q-table in an episode. 
These two factors create more variety in the achieved solutions and by retaining the best out of them, it is easy to see how we might arrive earlier to a viable solution.
\\
However, the other side of this described coin is that, with less stability provided by GA, comes a harder time in making specific and iterative changes to a solution.
Looking at Figure \ref{figAVG}, it is very interesting to note these very similar performances between the two algorithms at 10 to 20 seconds of training, before RL jumps much higher while GA keeps plateau-ing at the same level.
An explanation to the phenomenon might be that RL iteratively tweeks the same solution to perform better which allows to fix encountered problems. Meanwhile, GA might provide too much change over each generation to slowly improve an existing solution further, explaining the flattening out of performance.

\subsubsection{Angle magnitude difference.}
Another difference between the two approach is the behavior of the agent during the game itself.
This was only made visible once GIFs were generated to view sample games, as resumed in Figure \ref{fig:framesRL} for the reinforcement learning agent and Figure \ref{fig:framesGA} for the genetic algorithm last generation. 
It became apparent that the RL trained agent was taking way wider swings and generally playing more "on the edge" than the GA trained agent who is keeping the pole more straight and moving more cautiously.
This initial obsevation was confirmed quantitatively, by measuring the average absolute value of the angle of the pole during every episode of training.
Indeed, it turns out that the average absolute value of the angle is consistently two orders of magnitude greater during RL training than during GA training. ($ \sim 0.05$ radians to $\sim 0.0005$ radians).
\\
Since it doesn't penalize exploratory moves, Q-Learning specifically is known to be more prone to taking risks than other RL techniques. 
However, we tried using SARSA (another RL method without this feature) and still confirmed the same results.
\\
This observation may be even more surprising considering that we know the RL agent clearly outperforms the Genetic Algorithm, we might intuitively think that keeping the pole more straight would reflect better overall play.


% GA FRAMES %

\begin{figure}[H]
	\centering
	\begin{subfigure}
		\centering
		\includegraphics[width=0.3\linewidth]{Images/frames/GA/6.png}
	\end{subfigure}
	\hfill
	\begin{subfigure}
		\centering
		\includegraphics[width=0.3\linewidth]{Images/frames/GA/9.png}
	\end{subfigure}
	\hfill
	\begin{subfigure}
		\centering
		\includegraphics[width=0.3\linewidth]{Images/frames/GA/12.png}
	\end{subfigure}
	\\
	\begin{subfigure}
		\centering
		\includegraphics[width=0.3\linewidth]{Images/frames/GA/15.png}
	\end{subfigure}
	\hfill
	\begin{subfigure}
		\centering
		\includegraphics[width=0.3\linewidth]{Images/frames/GA/18.png}
	\end{subfigure}
	\hfill
	\begin{subfigure}
		\centering
		\includegraphics[width=0.3\linewidth]{Images/frames/GA/21.png}
	\end{subfigure}
	\caption{Frames 6, 9, 12, 15 and 21 of a game played by the GA algorithm}
	\label{fig:framesGA}
\end{figure}

% RL FRAMES %
\begin{figure}[H]
	\centering
	\begin{subfigure}
		\centering
		\includegraphics[width=0.3\linewidth]{Images/frames/RL/6.png}
	\end{subfigure}
	\hfill
	\begin{subfigure}
		\centering
		\includegraphics[width=0.3\linewidth]{Images/frames/RL/9.png}
	\end{subfigure}
	\hfill
	\begin{subfigure}
		\centering
		\includegraphics[width=0.3\linewidth]{Images/frames/RL/12.png}
	\end{subfigure}
	\\
	\begin{subfigure}
		\centering
		\includegraphics[width=0.3\linewidth]{Images/frames/RL/15.png}
	\end{subfigure}
	\hfill
	\begin{subfigure}
		\centering
		\includegraphics[width=0.3\linewidth]{Images/frames/RL/18.png}
	\end{subfigure}
	\hfill
	\begin{subfigure}
		\centering
		\includegraphics[width=0.3\linewidth]{Images/frames/RL/21.png}
	\end{subfigure}
	\caption{Frames 6, 9, 12, 15 and 21 of a game played by the RL agent}
	\label{fig:framesRL}
\end{figure}

Figures \ref{fig:framesGA} and \ref{fig:framesRL} show that the GA algorithm keeps the pole very straight with very little movement.
Meanwhile, the RL agent makes the pole more angled, using a wider swing.

\subsection{Final model comparison}

The second approach used to compare the obtained results is based on the final models' observations.

\subsubsection{State-Action table}

Figure \ref{figTABLEDIFF} shows the difference between the two obtained state-action tables of the final reinforcement learning agent and one of the individuals from the last generation of the genetic algorithm.

Since the \textit{Q-Table} does not provide an exact chosen action given a state, the compared state-action table of the reinforcement learning agent used in Figure \ref{figTABLEDIFF} is obtained by selecting the action $a$ of the state $s$ based on $argmax_{a_i} (Q(s,a_i))$.

\begin{figure}[H]
	\centering
	\includegraphics [width=\linewidth]{Images/diff_zoomed.png}
	\caption{Difference between the two state-action tables obtained using GA and RL. The black pixels represent the states where the chosen action is the same, while white pixels represent a different action choice. The x-axis contains all the possible pairs of the first two elements of the state's vector, $(s_1,s_2)$, and the y-axis contains all the possible pairs of the last two elements, $(s_3,s_4)$. }
	\label{figTABLEDIFF}
\end{figure}


Looking into Figure \ref{figTABLEDIFF}, it is possible to observe how the actions selected by the two algorithms differ in various states. However, some wide areas where the action taken by both algorithms is the same can be spotted, which could be referred to as sensitive states where considering a different choice could lead the pole to fall down.\\
Figure \ref{figTABLEDIFF} also points out that the two algorithm were able to find two valid and different solutions on the same problem.

\begin{figure}[H]
	\centering
	\includegraphics [width=\linewidth]{Images/diff_comparison.png}
	\caption{Evolution of the state-action table of both models: on the left, the differences of the final state-action table, on the right, the differences among the state-action table on the two models before the training.}
	\label{figDIFF_COMPARISON}
\end{figure}


Figure \ref{figDIFF_COMPARISON} shows the evolution of the state-action table of the two models, before and after the training. The resulting table differes in several points from the starting one, Figure \ref{figDIFF_COMPARISON} highlights one section where the two algorithm decided to performed the opposite actions. 

\subsubsection{Testing performances}

Figure \ref{figRLvsGA} shows the final testing results of the two methods. As already showed by the traning comparison in both Figure \ref{figAVG} and Figure \ref{figMAX}, The reinforcement learning agent showed a better generalization ability and more understanding of the environment, leading to obtain better results.\\
The testing has been conducted as follow: the environment has been seeded with ten different seeds and for every execution the same environment has been provided to both the algorithm.
The reinforcement learning agent has been tested using the obtained \textit{q-table} from the training, the genetic algorithm population is the last generation of the training performed.\\
Every individual of the GA population have been tested for all the tests, and the best one has been considered for the comparison, during the testing of the population, no particular difference have been noticed between the reward obtained by the individuals.
\begin{figure}[H]
	\centering
	\includegraphics [width=0.5\textwidth]{Images/RL_vs_GA.png}
	\caption{The plot shows the results obtained testing the two models on the same test set. The x-axis represents the number of the test set, the y-axis the number of steps the model was able to take before reaching the goal. The blue line represents the results obtained using the model trained with the GA, the red line the results obtained using the model trained with the RL.}
	\label{figRLvsGA}
\end{figure}
Observing the results, some differences regarding the variance can be noticed: the GA population seem to obtain more consistent results in different tests, but due the fact that the testing is performed on the entire generation this could be in part related to the fact that every time the best individual is extracted, even if analyzing the last generation of the GA algorithm, most of the individual scored similar results.\\