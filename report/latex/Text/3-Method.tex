\section{Method}
To evaluate the performance of Reinforcement Learning and Genetic Algorithms,
experiments have been conducted on different environments:
Cart Pole and \textbf{....}.

Below, we describe the environments and the implementation used in the experiments.
\subsection{Cart Pole}
Cart Pole is a classic control problem in reinforcement learning. 
The goal is to balance a pole on a cart that can move left or right. 

The state space is four-dimensional, consisting of the cart position, cart velocity, pole angle, and pole angular velocity $[p, v, \alpha, \omega]$. 

The action space is discrete, with two possible actions: move left or move right. 

The reward is 1 for every time step the pole is balanced.

The goal is to balance the pole for as long as possible, with a limit of 500 actions.

The environment, called \textit{CartPole-v1}, is implemented in Python using the Gymnasium library \cite{towers_gymnasium_2023}.

\subsubsection{Reinforcement Learning}
The reinforcement learning implementation is based on temporal difference learning \cite{sutton1998temporal}, 
in particular Q-learning.
The implementation takes inspiration on the work of \textit{JackFurby} \cite{JackFurbyCartPole}.

The \textit{Q-table} is represented by the discretization of the continuous 4-dimensional state vector in 20 even intervals for every dimension of the vector leading to 40 possible pairs of $<state,action>$, considering the two possible actions.
Once an action is performed, the state selected is the first larger that the observed state.\\
The parameters used in the experiments are the following:
\begin{itemize}
	\item Learning rate $\alpha = 0.1$
	\item Discount factor $\gamma = 0.95$
	\item Number of episodes $n_{ep} = 1000$
	\item Exploration rate $\varepsilon$ starts with $\varepsilon(0)=1$ and decays by $\varepsilon(t) = \varepsilon(t - 1) - \frac{1}{\frac{n_{ep}}{2} - 1}$, every episode, stopping after $\frac{n_{ep}}{2}$ episodes.
	\item Penalty factor $PF = -375$
\end{itemize}

\subsubsection{Genetic Algorithms}

\textbf{Genotype}\\
Since Genetic Algorithms can be very different depending on the genotype chosen to represent individuals, we have tried several different implementation of GA, varying the used genotype.
\\
The first method we used is a very naive implementation that can be applied to a very large variety of problems with GA : reprensenting individuals with the vector of all actions they will perform in order.
Thus, genotype[i] corresponds to move number i performed by the corresponding individual.
In this apporach, mutation is performed by changing an action in the genotype from left to right or from right to left.
\\
Since this particular genotype doesn't generalize well to the random initialization of the position of the pole, we have also tried this in fixed conditions, by seeding the environment to always start in the same place.
\\
Second, we tried another GA implementation, relying on what we already were using in our Reinforcement Learning part, that is the \textit{Q-table}.
In this approach, we don't try to predict every action one by one as we did previously but instead use GA to assign values to state-actions pairs in our \textit{Q-table} and then act greedily by selecting the best action for every state we encounter.
Here, mutation is performed by adding some random noise to the \textit{Q-table} values.
\\

\textbf{Parameters}\\
The GA parameters can be found in the following table :
\begin{tabular}
	Genotype & Population Size & Generations & Selection & Mutation Rate & Crossover \\
	\textit{Q-table} & 50 & 200 & Rank & 0.05 & One-point \\
  \end{tabular}