\section{Introduction}
% Delete the text and write your Introduction here:
%------------------------------------



In the field of Machine learning, several methods can be used to solve the same problem. For example, trying to find the local minima using evolutionary algorithms or supervised learning in the form of backpropagation. Both come with advantages and disadvantages depending on if we are after precision or just want to find the local minima fast, but also depending on the nature of our problem.

More specifically interesting is the comparison of the performance of Genetic Algorithms (GA) and Reinforcement Learning (RL) techniques in the context of a game environment. On one hand, in reinforcement learning the agent engages a dynamic and evolving environment by taking actions that affect it to accomplish a specific job. On the other hand, we have evolutionary algorithms that employ evolutionary principles for automated and concurrent problem-solving by drawing inspiration from populations of interacting organisms. Despite their apparent dissimilarities, RL and GA both tackle the same fundamental issue: optimizing a function. This entails maximizing an agent's reward in RL and the fitness function in evolutionary algorithms, respectively, particularly in environments where the parameters may be unknown \cite{drugan2019reinforcement}. 

This paper focuses on comparing Reinforcement learning and Genetic Algorthms by having them balance a cartpole in 500 moves. More specifically it is a problem in nonlinear dynamics where an inverted pendulum is balancing in a cart. The aim or final goal of both RL and GA are to keep it the system balanced until they run out of moves. the environment will be described in more detail under the environment part.  
\begin{figure}[H]
    \centering
    \includegraphics [scale = 0.18]{Images/cartpole.png}
    \caption{the cartpole in 2D graphics}
    \label{figRL}
\end{figure}
Previous work in the comparison of GA versus RL include \cite{drugan2019reinforcement} which focuses on a comperhensive overview of recent trends in the feild rather than comparisons of subclasses of algorithms or particular aspects of RL and GA. There a re also several works which focus on combining these two methods for mahcine learning by either using GA to train RL or vice versa such as \cite{eiben2007reinforcement} where the authurs try to use Reingforcment learnign to tune the parameters of GA. The opossite combination of training RL using GA has also been explored \cite{khadka2018evolutionary}.



