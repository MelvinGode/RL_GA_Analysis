\section{Discussion}
% Delete the text and write your Discussion here:
%------------------------------------
Originally, when only trying the action-by-action approach in GA, the dependency of the genotype on the initial state led to poor performances for GA implementation and no real generalization capacities.
However, it was very interesting to see that the Genetic Algorithm performed much better when combining it with features inspired from Reinforcement Learning - namely the \textit{Q-table}.
\\
One thing that we also realized while working on this project is also the fact that the parameter search space is much larger in GA than for its RL counterpart. 
While RL appears to be doing approximately the same thing at a difference pace depending on its parameters, it can feel like GA behaves in very different ways depending on the chosen combination of parameters.
While probably not very determining in the final results, this could also partly explain why we did not manage to achieve the same performances with GA as witn RL.
\\

\subsection{Future work}
A few ideas that we had which could probably improve GA's performance would be to try to target mutations more efficiently.
First, we could increase the probability of mutation for low scoring individuals (note that we would then have to play the game twice per generation to measure fitness after crossover).
This is not a new idea in Genetic Algorithms and is a very reasonable thing to do.
\\
Second, thinking about the fact that RL is better at targeting the areas it needs to improve on, we thought about increasing the probability of mutation, specifically for states close to where individuals often lose.
This could help "focusing" on what is wrong with our genotype and performing smarter mutations.
\\
However it is important to keep in mind that, for Genetic Algorithms, the crossover is the most important search operator, while mutation is only supposed to bring some diversity and little nudges in the search.
So we should not expect too much out of improving the mutation operation as it is not the main point of the algorithm.
\\
It is also good to keep in mind that the obtained results could be possible thanks to \textit{state discretization}, which allowed us to use the aforementioned tabular approach.
Without the state discretization approach, different and more elaborate directions should have been considered. 

% note: i commented this part since ive included something regarding neat networks in the method section (explaining why we did not used them) 

%A discussed alternative could be exploring the path of function approximation, which is a very common approach in continuous state problems in RL, 
%but no further experiments have been conducted in this direction due to the complex adaptations of this technique to the GA algorithm (see \cite[NEAT]{stanley2002NEAT} for example).
