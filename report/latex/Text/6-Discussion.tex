\section{Discussion}
% Delete the text and write your Discussion here:
%------------------------------------
Originally, when only trying the action-by-action approach in GA, observations led to one-sided results due to the dependency of the genotype on the initial state, leading to poor performances for GA implementation and no real generalization capacities.
However, it was very interesting to see that the Genetic Algorithm performed much better when combining it with features from Reinforcement Learning - namely the \textit{Q-table}.

The obtained results could be possible thanks to the \textit{state discretization}, which allowed us to use the aforementioned tabular approach.
Without the state discretization approach, different and more elaborate directions should have been considered. 
A discussed alternative could be exploring the path of function approximation, which is a very common approach in continuous state problems in RL, 
but no further experiments have been conducted in this direction due to the complex adaptations of this technique to the GA algorithm.
