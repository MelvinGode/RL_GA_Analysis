\section{Discussion}
% Delete the text and write your Discussion here:
%------------------------------------
Originally, when only trying the action-by-action approach in GA, the dependency of the genotype on the initial state led to poor performances for GA implementation and no real generalization capacities.
However, it was very interesting to see that the Genetic Algorithm performed much better when combining it with features inspired from Reinforcement Learning - namely the \textit{Q-table}.
\\
One thing that we also realized while working on this project is also the fact that the parameter search space is much larger in GA than for its RL counterpart. 
While RL appears to be doing approximately the same thing at a difference pace depending on its parameters, it can feel like GA behaves in very different ways depending on the chosen combination of parameters.
While probably not very determining in the final results, this could also partly explain why we did not manage to achieve the same performances with GA as witn RL.
\\
It is also good to keep in mind that the obtained results could be possible thanks to \textit{state discretization}, which allowed us to use the aforementioned tabular approach.
Without the state discretization approach, different and more elaborate directions should have been considered. 

% note: i commented this part since ive included something regarding neat networks in the method section (explaining why we did not used them) 

%A discussed alternative could be exploring the path of function approximation, which is a very common approach in continuous state problems in RL, 
%but no further experiments have been conducted in this direction due to the complex adaptations of this technique to the GA algorithm (see \cite[NEAT]{stanley2002NEAT} for example).
