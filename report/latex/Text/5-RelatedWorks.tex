\section{Related Works}

% add sources
Similar comparisons between RL and GA have been made by \cite{drugan2019reinforcement},\cite{taylor2006comparing} and \cite{pollack1997coevolution}. \cite{drugan2019reinforcement}  focuses more on a comprehensive overview of recent trends in the field rather than comparing subclasses of algorithms or particular aspects of RL and GA and states that both can be good at solving RL problems. \cite{pollack1997coevolution} successfully trained an evolutionary algorithm to play backgammon, a typical RL type game. \cite{moriarty1996efficient} presents a reinforcement learning technique SANE(Symbiotic, Adaptive Neuro-Evolution) which uses a genetic algorithm to evolve a population of neurons to perform a task. The evaluation which was done using the inverted pendulum problem in the form of a cart pole. Using the same problem it was compared to Q learning and was found to be 2 times faster. Several works focus on combining these two methods for machine learning by either using GA to train RL or vice versa such as \cite{eiben2007reinforcement} where the authors try to use Reinforcement learning to tune the parameters of GA.  Papers such as  \cite{khadka2018evolutionary} explore the opposite combination of training RL using GA . It is important to mention that the implementation of the reinforcement learning algorithm that is used is based on the work of \textit{JackFurby} \cite{JackFurbyCartPole}. 

In our investigation into various genotype options for addressing the pole-balancing problem, we consciously opted not to explore a solution based on NEAT\cite{stanley2002NEAT} networks, build a neural network able to predict the correct action given the environment state as input.\\
Considering the simplicity of the chosen environment and given our primary objective of comparing genetic algorithm (GA) approaches with reinforcement learning (RL) agents, we excluded the possibility to employ a NEAT network for such environment.

Ensuring a fair and transparent comparison between GA and RL methods is the main objective. Introducing unnecessary complexity through a NEAT network could potentially obscure the true comparative performance of the two approaches. Thus, we chose simpler solutions, aligning more closely with typical RL agent implementations. This approach facilitates a clearer evaluation of the relative effectiveness and efficiency of GA and RL algorithms in the pole-balancing task.