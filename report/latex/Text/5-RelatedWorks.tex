\section{Related Works}

% add sources
Similar comparisons between RL and GA have been made by \cite{drugan2019reinforcement},\cite{taylor2006comparing} and \cite{pollack1997coevolution}. \cite{drugan2019reinforcement}  focuses more on a comprehensive overview of recent trends in the field rather than comparing subclasses of algorithms or particular aspects of RL and GA and states that both can be good at solving RL problems. \cite{pollack1997coevolution} successfully trained an evolutionary algorithm to play backgammon, a typical RL type game. \cite{moriarty1996efficient} presents a reinforcement learning technique SANE((Symbiotic, Adaptive Neuro-Evolution)) which uses a genetic algorithm to evolve a population of neurons to perform a task. The evaluation which was done using the inverted pendulum problem in the form of a cart pole. Using the same problem it was compared to Q learning and was found to be 2 times faster. Several works focus on combining these two methods for machine learning by either using GA to train RL or vice versa such as \cite{eiben2007reinforcement} where the authors try to use Reinforcement learning to tune the parameters of GA.  Papers such as  \cite{khadka2018evolutionary} explore the opposite combination of training RL using GA . It is important to mention that the implementation of the reinforcement learning algorithm that is used is based on the work of \textit{JackFurby} \cite{JackFurbyCartPole}. 

